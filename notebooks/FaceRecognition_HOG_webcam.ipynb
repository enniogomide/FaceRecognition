{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Face recognition**\n",
        "\n",
        "the python library was used for this work:\n",
        "**face_recognition.py**\n",
        "\n",
        "to run requires GPU. it will install de library every time that will be run.\n",
        "\n",
        "To use this notebook, you need to have run the notebook: “FaceRecognition_HOG.ipynb”, as it generates the data to be used as a parameter for comparing the photos taken by the webcam.\n",
        "\n",
        "Will load the files with the names and encoding data of the previously generated photos.\n",
        "\n",
        "Based on this, it identifies whether it is someone you know or not.\n",
        "\n",
        "due to time constraints, it wasn't possible to include several images on the webcam to validate. validation takes place one by one.\n",
        "\n",
        "it was not implemented as a video capture, which is also possible, but would require further adjustments.\n"
      ],
      "metadata": {
        "id": "lLDavn2JXq0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g770Jw-aXb9L",
        "outputId": "0d6971bc-df6e-492d-fc20-a4bc71ce5aaa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (8.1.8)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (19.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from face_recognition) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from face_recognition) (11.1.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566162 sha256=f36e575b8e403f58a87306864c3cd3b82fdc1cf5b2087757021056e4fd4a32ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/52/ec/9355da79c29f160b038a20c784db2803c2f9fa2c8a462c176a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "# Install de library for face recogntinio\n",
        "#\n",
        "!pip3 install face_recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To import the face_recognition it is required to have GPU."
      ],
      "metadata": {
        "id": "uMm1srTWP38u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import face_recognition"
      ],
      "metadata": {
        "id": "mDJYi6edPwXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qEJvGzPgPz1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kRJp5Cph_7aF",
        "outputId": "ea6a9172-55c0-418e-ee4a-f7c09ec4c73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chage to the FacaRecogniton folder created in the clone process.\n",
        "\n",
        "%cd '/content/drive/MyDrive/FaceRecognition'\n",
        "!pwd\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM7J_OAEA2yY",
        "outputId": "b0adeec4-ab36-4717-e00a-9bd00f94f4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FaceRecognition\n",
            "/content/drive/MyDrive/FaceRecognition\n",
            "total 915\n",
            "-rw------- 1 root root 835954 Jan 22 18:44 encoding_faces_with_class.csv\n",
            "-rw------- 1 root root  10537 Jan 22 18:35 faces_encoding_parameters.csv\n",
            "drwx------ 2 root root   4096 Jan 20 18:11 ImagesTrain\n",
            "drwx------ 2 root root   4096 Jan 20 18:11 ImagesValidation\n",
            "-rw------- 1 root root  80398 Jan 24 20:58 img.jpg\n",
            "-rw------- 1 root root     45 Jan 22 18:35 names_of_faces.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the file with the faces with the encoding\n",
        "\n",
        "current_folder = os.getcwd()\n",
        "file_url = current_folder + \"/faces_encoding_parameters.csv\"\n",
        "faces_loaded = pd.read_csv(file_url, header=None)\n",
        "faces_encoding = pd.DataFrame(faces_loaded).to_numpy().tolist()\n"
      ],
      "metadata": {
        "id": "0BZ35os-O5ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the file with the faces names\n",
        "\n",
        "current_folder = os.getcwd()\n",
        "file_url = current_folder + \"/names_of_faces.csv\"\n",
        "names_loaded = pd.read_csv(file_url, header=None)\n",
        "faces_names = pd.DataFrame(names_loaded).to_numpy().tolist()"
      ],
      "metadata": {
        "id": "5iSQvptXQiKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(faces_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spmkL1WLRaft",
        "outputId": "c924a5b0-bb5a-4a7d-f4bb-bb74d34cb8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Face recogntion using the webcam**\n",
        "\n",
        "To build this functionality, we used an example from the library: face_recognition\n",
        "\n",
        "link: https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py\n",
        "\n",
        "It uses the webcam for reading, and the result of the previous process.\n",
        "\n",
        "If you are only going to run this part of the recognition, you must import the name and enconding files that were previously copied."
      ],
      "metadata": {
        "id": "bbJ5HC6gZ0Pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To remember\n",
        "for face recogntion are use:\n",
        "faces_names\n",
        "faces_encoding\n",
        "The two files are attached to the material.\n",
        "\n",
        "And the first stept must be run."
      ],
      "metadata": {
        "id": "htvKcxCabLaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to recognize faces using the webcam\n",
        "# link: https://github.com/theAIGuysCode/colab-webcam/blob/main/colab_webcam.ipynb\n",
        "\n",
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "FmpIwqwrnbUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "Zd5dh3lWnxAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "metadata": {
        "id": "C6HHiKe1n3dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data)\n",
        "  # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
        "  rgb_small_frame = img[:, :, ::-1]\n",
        "\n",
        "  # Find all the faces and face encodings in the current frame of video\n",
        "  face_locations = face_recognition.face_locations(rgb_small_frame)\n",
        "\n",
        "  face_encodings = face_recognition.face_encodings(img, face_locations)[0]\n",
        "  matches = face_recognition.compare_faces(faces_encoding, face_encodings)\n",
        "\n",
        "  matches = face_recognition.compare_faces(faces_encoding, face_encodings)\n",
        "  font = cv2.FONT_HERSHEY_DUPLEX\n",
        "  if matches[matches.index(True)]:\n",
        "    name=str(faces_names[matches.index(True)])\n",
        "    name = name[name.find(\"[\") + 2:name.find(\"]\")-1]\n",
        "  else:\n",
        "    name = \"Desconhecido\"\n",
        "\n",
        "\n",
        "  # grayscale img\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  # get face bounding box coordinates using Haar Cascade\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # draw face bounding box on image\n",
        "  for (x,y,w,h) in faces:\n",
        "      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "      img = cv2.putText(img, name, (x + 6, y - 6), font,\n",
        "                   1.0, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "  # save image\n",
        "  # cv2_imshow(img)\n",
        "  filename = name + \".jpg\"\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "\n",
        "  return filename"
      ],
      "metadata": {
        "id": "xO-IMvI-n-2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print(f\"Saved to {filename}\")\n",
        "\n",
        "# Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8vDCgXtkoF3H",
        "outputId": "5450a6a9-04f9-4b03-b3d1-0a66e9ce9108",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True is not in list\n"
          ]
        }
      ]
    }
  ]
}